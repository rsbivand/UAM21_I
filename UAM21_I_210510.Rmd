---
title: "Introduction to R (III)"
author: "Roger Bivand"
date: "Monday 10 May 2021"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
theme: united
bibliography: uam21.bib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, paged.print=FALSE)
```

```{r set-options-cdn, echo=TRUE, results='hide'}
td <- tempfile()
dir.create(td)
Sys.setenv("PROJ_USER_WRITABLE_DIRECTORY"=td)
#Sys.setenv("PROJ_NETWORK"="ON")
options("rgdal_show_exportToProj4_warnings"="none")
```

### Copyright

All the material presented here, to the extent it is original, is available under [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/). 

### Required current contributed CRAN packages:

I am running R 4.0.5, with recent `update.packages()`.

```{r, echo=TRUE}
needed <- c("RSQLite", "rgdal", "units", "gdistance", "igraph", "stars", "raster", 
"terra", "sp", "mapview", "ggplot2", "mapsf", "tmap", "colorspace", "RColorBrewer", 
"sf", "classInt")
```

### Script

Script and data at https://github.com/rsbivand/UAM21_I/raw/main/UAM21_I_210510.zip. Download to suitable location, unzip and use as basis.


### Schedule

- Today, R as a GIS, planar and spherical geometries, projections and transformations

| Time | Topic |
| :--- | :---- |
|**Wednesday 5/5**|    |
|09.00-12.00| What is R: programming language, community, ecosystem? What may it be used for in analysing spatial data in a social science setting? What are the basic data structures in R? How can we start writing an R markdown notebook? How to access help in using R? How to use built-in data sets and why? How to write reproducible examples? What can we learn from code examples? How can R help us in furthering reproducible research? |
|13.00-16.00| What kinds of  data objects are used in R? What is the structure of a data.frame? What is a list object? What kinds of data can be contained in data objects? |
|**Thursday 6/5**|    |
|09.00-12.00| How may we read data into R? From files, including spatial data files, and from online resources? How can we choose between output formats for notebooks and other output media? How can one choose between the basic graphics functions and devices in R? |
|13.00-16.00| When our data include spatial data objects, in which ways may they be represented in R? How can one make simple thematic maps using R? (sf, stars, tmap) |
|**Monday 10/5**|    |
|09.00-12.00| How can we use class intervals and colour palettes to communicate? Rather than "lying with maps", how can we explore the impact of choices made in thematic cartography? How can we condition on continuous or discrete variables to permit visual comparison? How can we combine multiple graphical elements in data visualization? (classInt, sf, tmap, mapsf) May we use R "like a GIS"? How may we structure temporal and spatio-temporal data? Closer introduction to R-spatial (sf, stars, gdalcubes, terra, GDAL, GEOS) |
|13.00-16.00| Planar and spherical geometries, projections and transformations (s2, PROJ, tmap, mapview, leaflet, geogrid) |
|**Tuesday 11/5**|    |
|09.00-12.00| Doing things with spatial data ... (osmdata, ...)  |
|13.00-16.00| Doing things with spatial data ... (osmdata, ...) |
|**Thursday 20/11**|    |
|09.00-12.00| Presentations/consultations/discussion |
|13.00-16.00| Presentations/consultations/discussion |




# Class intervals

**classInt** provides the key class interval determination for thematic mapping of continuous variables. The `classIntervals()` function takes a numeric vector (now also of classes POSIXt or units), a target number of intervals, and a style of class interval. Other arguments control the closure and precision of the intervals found.

```{r, echo=TRUE}
library(classInt)
args(classIntervals)
```
Lapa et al. [-@LAPA2001] (Leprosy surveillance in Olinda, Brazil, using spatial analysis techniques) made available the underlying data set of Olinda census tracts (setor) in the Corrego Alegre 1970-72 / UTM zone 25S projection (EPSG:22525). Marilia SÃ¡ Carvalho and I wrote a [tutorial](https://rsbivand.github.io/geomed19-workshop/olinda.pdf) in 2003/4 based on this data set; there is more information in the tutorial.

We'll find 7 intervals using Fisher natural breaks for the deprivation variable:

```{r, echo=TRUE}
library(sf)
olinda_sirgas2000 <- st_read("../data/olinda_sirgas2000.gpkg")
(cI <- classIntervals(olinda_sirgas2000$DEPRIV, n=7, style="fisher"))
```

```{r, echo = TRUE, eval=TRUE} 
Sys.setenv("PROJ_NETWORK"="OFF")
sf_proj_network()
list.files(sf_proj_search_paths()[1])
sf_extSoftVersion()
```

We also need to assign a palette of graphical values, most often colours, to use to fill the intervals, and can inspect the intervals and fill colours with a plot method:

The **RColorBrewer** package gives by permission access to the ColorBrewer palettes accesible from the [ColorBrewer](http://colorbrewer2.org)
website. Note that ColorBrewer limits the number of classes tightly, only 3--9 sequential classes


```{r, echo=TRUE}
library(RColorBrewer)
pal <- RColorBrewer::brewer.pal((length(cI$brks)-1), "Reds")
plot(cI, pal)
```

We can also display all the ColorBrewer palettes:

```{r, echo=TRUE}
display.brewer.all()
```

Try exploring alternative class interval definitions and palettes, maybe also visiting http://hclwizard.org/ and its `hclwizard()` Shiny app, returning a palette generating function on clicking the "Return to R" button:

```{r, echo=TRUE}
library(colorspace)
hcl_palettes("sequential (single-hue)", n = 7, plot = TRUE)
```

```{r, echo=TRUE, eval=FALSE}
pal <- hclwizard()
pal(6)
```

The end of rainbow discussion is informative:

```{r, echo=TRUE}
wheel <- function(col, radius = 1, ...)
  pie(rep(1, length(col)), col = col, radius = radius, ...) 
opar <- par(mfrow=c(1,2))
wheel(rainbow_hcl(12))
wheel(rainbow(12))
par(opar)
```

See recent [R blog](https://developer.r-project.org/Blog/public/2019/11/21/a-new-palette-for-r/index.html).

See also treatments in [Fundamentals of Data Visualization](https://serialmentor.com/dataviz/).


# Thematic mapping

The **sp** package provided base graphics plot and image methods. **sf** provides plot methods using base graphics; the method for `"sf"` objects re-arranges the plot window to provide a colour key, so extra steps are needed if overplotting is needed:

```{r, echo=TRUE}
plot(olinda_sirgas2000[,"DEPRIV"], breaks=cI$brks, pal=pal)
```

(returns current `par()` settings); the method also supports direct use of **classInt**:

```{r, echo=TRUE}
plot(olinda_sirgas2000[,"DEPRIV"], nbreaks=7, breaks="fisher", pal=pal)
```

Earlier we used the plot method for `"sfc"` objects which does not manipulate the graphics device, and is easier for overplotting.


### The tmap package

**tmap**: Thematic maps show spatial distributions. The theme refers to the phenomena that is shown, which is often demographical, social, cultural, or economic. The best known thematic map type is the choropleth, in which regions are colored according to the distribution of a data variable. The R package tmap offers a coherent plotting system for thematic maps that is based on the layered grammar of graphics. Thematic maps are created by stacking layers, where per layer, data can be mapped to one or more aesthetics. It is also possible to generate small multiples. Thematic maps can be further embellished by configuring the map layout and by adding map attributes, such as a scale bar and a compass. Besides plotting thematic maps on the graphics device, they can also be made interactive as an HTML widget. In addition, the R package **tmaptools** contains several convenient functions for reading and processing spatial data. See  [@JSSv084i06] and Chapter 8 in [@geocompr].

The **tmap** package provides cartographically informed, grammar of graphics (gg) based functionality now, like **ggplot2** using **grid** graphics. John McIntosh tried with [ggplot2](http://johnmackintosh.com/2017-08-22-simply-mapping/), with quite nice results. I suggested he look at **tmap**, and things got [better](http://johnmackintosh.com/2017-09-01-easy-maps-with-tmap/), because **tmap** can switch between interactive and static viewing. **tmap** also provides direct access to **classInt** class intervals. 

```{r, echo=TRUE}
library(tmap)
tmap_mode("plot")
o <- tm_shape(olinda_sirgas2000) + tm_fill("DEPRIV", style="fisher", n=7, palette="Reds")
class(o)
```

returns a `"tmap"` object, a **grid** GROB (graphics object), with print methods.

```{r, echo=TRUE}
o
```

Since the objects are GROBs, they can be updated, as in **lattice** with **latticeExtra** or **ggplot2**:

```{r, echo=TRUE}
o + tm_borders(alpha=0.5, lwd=0.5)
```


There is also a Shiny tool for exploring palettes:

```{r, echo=TRUE, eval=FALSE}
tmaptools::palette_explorer()
```

### The mapsf, formerly cartography package

**cartography** helped to design cartographic representations such as proportional symbols, choropleth, typology, flows or discontinuities maps. It also offers several features that improve the graphic presentation of maps, for instance, map palettes, layout elements (scale, north arrow, title...), labels or legends. [@giraud+lambert16; @giraud+lambert17], http://riatelab.github.io/cartography/vignettes/cheatsheet/cartography_cheatsheet.pdf. The package is associated with **rosm**: Download and plot Open Street Map <http://www.openstreetmap.org/>, Bing Maps <http://www.bing.com/maps> and other tiled map sources. Use to create basemaps quickly and add hillshade to vector-based maps. https://cran.r-project.org/web/packages/rosm/vignettes/rosm.html. **mapsf** has followed up on suggestions to improve **cartography**, using recent changes in base R colour handling: https://developer.r-project.org/Blog/public/2019/11/21/a-new-palette-for-r/index.html.

The package organizes extra palettes:

```{r, echo=TRUE}
library(mapsf)
cols <- mf_get_pal(n = c(5, 5), pal = c("Reds 2", "Greens"))
plot(1:10, rep(1, 10), bg = cols, pch = 22, cex = 4, axes=FALSE, ann=FALSE)
```

The plotting functions (not methods) use base graphics:

```{r, echo=TRUE}
mf_choro(olinda_sirgas2000, var="DEPRIV", breaks="fisher", nbreaks=7, pal=pal, leg_val_rnd=3)
```

(returns NULL)

### The ggplot2 package

The **ggplot2** package provides the `geom_sf()` facility for mapping:

```{r, echo=TRUE}
library(ggplot2)
```

```{r, echo=TRUE}
g <- ggplot(olinda_sirgas2000) + geom_sf(aes(fill=DEPRIV))
g
```

It is possible to set a theme that drops the arguably unnecessary graticule:

```{r, echo=TRUE}
g + theme_void()
```


```{r, echo=TRUE}
g + theme_void() + scale_fill_distiller(palette="Reds", direction=1)
```

but there is a lot of jumping through hoops to get a simple map. To get proper class intervals involves even more work, because **ggplot2** takes specific, not general, positions on how graphics are observed. ColorBrewer eschews continuous colour scales based on cognitive research, but ggplot2 enforces them for continuous variables (similarly for graticules, which may make sense for data plots but not for maps).


# Interactive maps

### **tmap** modes

Using `tmap_mode()`, we can switch between presentation (`"plot"`) and interactive (`"view"`) plotting:

```{r, echo=TRUE}
tmap_mode("view")
```

```{r, echo=TRUE}
o + tm_borders(alpha=0.5, lwd=0.5)
```


```{r, echo=TRUE}
tmap_mode("plot")
```


### The **mapview** package

**mapview**: Quickly and conveniently create interactive visualisations of spatial data with or without background maps. Attributes of displayed features are fully queryable via pop-up windows. Additional functionality includes methods to visualise true- and false-color raster images, bounding boxes, small multiples and 3D raster data cubes. It uses **leaflet** and other HTML packages.

```{r, echo=TRUE}
library(mapview)
if (sf:::CPL_gdal_version() >= "3.1.0") mapviewOptions(fgb = FALSE)
mapview(olinda_sirgas2000, zcol="DEPRIV", col.regions=pal, at=cI$brks)
```

## Further examples

```{r, echo=TRUE}
data("pol_pres15", package = "spDataLarge")
pol_pres15 <- st_buffer(pol_pres15, dist=0)
```


```{r, echo=TRUE}
library(tmap)
o <- tm_shape(pol_pres15) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.5, alpha=0.4) + tm_layout(panel.labels=c("Duda", "Komorowski"))
```

```{r, echo=TRUE, cache=TRUE}
o + tm_fill(c("I_Duda_share", "I_Komorowski_share"), n=6, style="pretty", title="I round\nshare of votes")
```


```{r, echo=TRUE, cache=TRUE}
o + tm_fill(c("II_Duda_share", "II_Komorowski_share"), n=6, style="pretty", title="II round\nshare of votes")
```

# Spatial data

Spatial data typically combine position data in 2D (or 3D), attribute data and metadata related to the position data. Much spatial data could be called map data or GIS data. We collect and handle much more position data since global navigation satellite systems (GNSS) like GPS came on stream 20 years ago, earth observation satellites have been providing data for longer.

```{r, echo = TRUE}
suppressPackageStartupMessages(library(osmdata))
library(sf)
```

```{r, cache=TRUE, echo = TRUE}
bbox <- opq(bbox = 'bergen norway')
byb0 <- osmdata_sf(add_osm_feature(bbox, key = 'railway',
  value = 'light_rail'))$osm_lines
tram <- osmdata_sf(add_osm_feature(bbox, key = 'railway',
  value = 'tram'))$osm_lines
byb1 <- tram[!is.na(tram$name),]
o <- intersect(names(byb0), names(byb1))
byb <- rbind(byb0[,o], byb1[,o])
saveRDS(byb, file="byb.rds")
```

Spatial vector data is based on points, from which other geometries are constructed. Vector data is often also termed object-based spatial data. The light rail tracks are 2D vector data. The points themselves are stored as double precision floating point numbers, typically without recorded measures of accuracy (GNSS provides a measure of accuracy). Here, lines are constructed from points.


```{r, echo = TRUE}
byb <- readRDS("byb.rds")
library(mapview)
mapviewOptions(fgb = FALSE)
mapview(byb)
```



## Advancing from the **sp** representation

### Representing spatial vector data in R (**sp**)


The **sp** package was a child of its time, using S4 formal classes, and the best compromise we then had of positional representation (not arc-node, but hard to handle holes in polygons). If we coerse `byb` to the **sp** representation, we see the formal class structure. Input/output used OGR/GDAL vector drivers in the **rgdal** package, and topological operations used GEOS in the **rgeos** package.


```{r, echo = TRUE}
library(sp)
byb_sp <- as(byb, "Spatial")
str(byb_sp, max.level=2)
```

```{r, echo = TRUE}
str(slot(byb_sp, "lines")[[1]])
```

```{r, echo = TRUE}
library(terra)
(byb_sv <- as(byb, "SpatVector"))
str(byb_sv)
```
```{r, echo = TRUE}
geomtype(byb_sv)
str(geom(byb_sv))
```

### Raster data

Spatial raster data is observed using rectangular (often square) cells, within which attribute data are observed. Raster data are very rarely object-based, very often they are field-based and could have been observed everywhere. We probably do not know where within the raster cell the observed value is correct; all we know is that at the chosen resolution, this is the value representing the whole cell area.

```{r, echo = TRUE, eval=FALSE}
library(elevatr)
elevation <- get_elev_raster(byb_sp, z = 10)
is.na(elevation) <- elevation < 1
saveRDS(elevation, file="elevation.rds")
```

```{r, echo = TRUE}
library(raster)
(elevation <- readRDS("elevation.rds"))
str(elevation, max.level=2)
```

```{r, echo=TRUE}
str(slot(elevation, "data"))
```

```{r, echo=TRUE}
str(as(elevation, "SpatialGridDataFrame"), max.level=2)
```

```{r, echo = TRUE, eval=TRUE, cache=TRUE}
mapview(elevation, col=terrain.colors)
```

```{r, echo = TRUE}
(elevation_sr <- as(elevation, "SpatRaster"))
str(elevation_sr)
```

```{r, echo = TRUE}
str(values(elevation_sr))
```

### Raster data

The **raster** package complemented **sp** for handling raster objects and their interactions with vector objects. 

It added to input/output using GDAL through **rgdal**, and better access to NetCDF files for GDAL built without the relevant drivers. 

It may be mentioned in passing that thanks to help from CRAN administrators and especially Brian Ripley, CRAN binary builds of **rgdal** for Windows and Apple Mac OSX became available from 2006, but with a limited set of vector and raster drivers. 

Support from CRAN adminstrators remains central to making packages available to users who are not able to install R source packages themselves, particularly linking to external libraries. 

Initially, **raster** was written in R using functionalities in **sp** and **rgdal** with **rgeos** coming later. 

It used a feature of GDAL raster drivers permitting the successive reading of subsets of rasters by row and column, permitting the processing of much larger objects than could be held in memory. 

In addition, the concepts of bricks and stacks of rasters were introduced, diverging somewhat from the **sp** treatment of raster bands as stacked columns as vectors in a data frame.

From this year, a new package called **terra** steps away from **sp** class representations, linking directly to GDAL, PROJ and GEOS.

### Questions arose

As **raster** evolved, two other packages emerged raising issues with the ways in which spatial objects had been conceptualized in **sp**. 

The **rgeos** package used the C application programming interface (API) to the C++ GEOS library, which is itself a translation of the Java Topology Suite (JTS). 

While the GDAL vector drivers did use the standard Simple Features representation of ector geometries, it was not strongly enforced. 

This laxity now seems most closely associated with the use of ESRI Shapefiles as a de-facto file standard for representation, in which many Simple Features are not consistently representable. 

### Need for vector standards compliance

Both JTS and GEOS required a Simple Feature compliant representation, and led to the need for curious and fragile adaptations. 

For example, these affected the representation of **sp** `"Polygons"` objects, which were originally conceptualized after the Shapefile specification: ring direction determined whether a ring was exterior or interior (a hole), but no guidance was given to show which exterior ring holes might belong to. 

As R provides a way to add a character string comment to any object, comments were added to each `"Polygons"` object encoding the necessary information. 

In this way, GEOS functionality could be used, but the fragility of vector representation in **sp** was made very obvious.

### Spatio-temporal data

Another package affecting thinking about representation was **spacetime**, as it diverged from **raster** by stacking vectors for regular spatio-temporal objects with space varying faster than time. 

So a single earth observation band observed repeatedly would be stored in a single vector in a data frame, rather than in the arguably more robust form of a four-dimensional array, with the band taking one position on the final dimension. 

The second edition of [@asdar2] took up all of these issues in one way or another, but after completing a spatial statistics special issue of the Journal of Statistical Software [@JSSv063i01], it was time to begin fresh implementations of classes for spatial data.

## Simple Features in R


It was clear that vector representations needed urgent attention, so the **sf** package was begun, aiming to implement the most frequently used parts of the specification [@iso19125; @kralidis08; @sfa]. 

Development was supported by a grant from the then newly started R Consortium, which brings together R developers and industry members. 

A key breakthrough came at the useR! 2016 conference, following an earlier decision to re-base vector objects on data frames, rather than as in **sp** to embed a data frame inside a collection of spatial features of the same kind. 

However, although data frame objects in S and R have always been able to take list columns as valid columns, such list columns were not seen as "tidy" [@JSSv059i10].

### Refresher: data frame objects

First, let us see that is behind the `data.frame` object: the `list` object. `list` objects are vectors that contain other objects, which can be addressed by name or by 1-based indices . Like the vectors we have already met, lists can be  accessed and manipulated using square brackets `[]`. Single list elements can be accessed and manipulated using double square brackets `[[]]`

Starting with four vectors of differing types, we can assemble a list object; as we see, its structure is quite simple. The vectors in the list may vary in length, and lists can (and do often) include lists


```{r , echo = TRUE}
V1 <- 1:3
V2 <- letters[1:3]
V3 <- sqrt(V1)
V4 <- sqrt(as.complex(-V1))
L <- list(v1=V1, v2=V2, v3=V3, v4=V4)
```



```{r , echo = TRUE}
str(L)
L$v3[2]
L[[3]][2]
```

Our `list` object contains four vectors of different types but of the same length; conversion to a `data.frame` is convenient. Note that by default strings are converted into factors:


```{r , echo = TRUE}
DF <- as.data.frame(L)
str(DF)
DF <- as.data.frame(L, stringsAsFactors=FALSE)
str(DF)
```


We can also provoke an error in conversion from a valid `list` made up of vectors of different length to a `data.frame`:


```{r , echo = TRUE}
V2a <- letters[1:4]
V4a <- factor(V2a)
La <- list(v1=V1, v2=V2a, v3=V3, v4=V4a)
DFa <- try(as.data.frame(La, stringsAsFactors=FALSE), silent=TRUE)
message(DFa)
```


We can access `data.frame` elements as `list` elements, where the `$` is effectively the same as `[[]]` with the list component name as a string:


```{r , echo = TRUE}
DF$v3[2]
DF[[3]][2]
DF[["v3"]][2]
```


Since a `data.frame` is a rectangular object with named columns with equal numbers of rows, it can also be indexed like a matrix, where the rows are the first index and the columns (variables) the second:


```{r , echo = TRUE}
DF[2, 3]
DF[2, "v3"]
str(DF[2, 3])
str(DF[2, 3, drop=FALSE])
```


If we coerce a `data.frame` containing a character vector or factor into a matrix, we get a character matrix; if we extract an integer and a numeric column, we get a numeric matrix.


```{r , echo = TRUE}
as.matrix(DF)
as.matrix(DF[,c(1,3)])
```

The fact that `data.frame` objects descend from `list` objects is shown by looking at their lengths; the length of a matrix is not its number of columns, but its element count:


```{r , echo = TRUE}
length(L)
length(DF)
length(as.matrix(DF))
```


There are `dim` methods for `data.frame` objects and matrices (and arrays with more than two dimensions); matrices and arrays are seen as vectors with dimensions; `list` objects have no dimensions:


```{r , echo = TRUE}
dim(L)
dim(DF)
dim(as.matrix(DF))
```


```{r , echo = TRUE}
str(as.matrix(DF))
```



`data.frame` objects have `names` and `row.names`, matrices have `dimnames`, `colnames` and `rownames`; all can be used for setting new values:


```{r , echo = TRUE}
row.names(DF)
names(DF)
names(DF) <- LETTERS[1:4]
names(DF)
str(dimnames(as.matrix(DF)))
```


R objects have attributes that are not normally displayed, but which show their structure and class (if any); we can see that `data.frame` objects are quite different internally from matrices:


```{r , echo = TRUE}
str(attributes(DF))
str(attributes(as.matrix(DF)))
```


If the reason for different vector lengths was that one or more observations are missing on that variable, `NA` should be used; the lengths are then equal, and a rectangular table can be created:


```{r , echo = TRUE}
V1a <- c(V1, NA)
V3a <- sqrt(V1a)
La <- list(v1=V1a, v2=V2a, v3=V3a, v4=V4a)
DFa <- as.data.frame(La, stringsAsFactors=FALSE)
str(DFa)
```


### Tidy list columns


```{r, echo = TRUE} 
DF$E <- list(d=1, e="1", f=TRUE)
str(DF)
```

At useR! in 2016, list columns were declared "tidy", using examples including the difficulty of encoding polygon interior rings in non-list columns. The decision to accommodate "tidy" workflows as well as base-R workflows had already been made, as at least some users only know how to use ``tidy'' workflows. 



### **sf** begins

[@RJ-2018-009] shows the status of the **sf** towards the end of 2017, with a geometry list column containing R wrappers around objects adhering to Simple Features specification definitions. The feature geometries are stored in numeric vectors, matrices, or lists of matrices, and may also be subject to arithmetic operations. Features are held in the `"XY"` class if two-dimensional, or `"XYZ"`, `"XYM"` or `"XYZM"` if such coordinates are available; all single features are `"sfg"` (Simple Feature geometry) objects: 

```{r, echo = TRUE} 
pt1 <- st_point(c(1,3))
pt2 <- pt1 + 1
pt3 <- pt2 + 1
str(pt3)
```

Geometries may be represented as "Well Known Text" (WKT):


```{r, echo = TRUE} 
st_as_text(pt3)
```

or as "Well Known Binary" (WKB) as in databases' "binary large objects" (BLOBs), resolving the problem of representation when working with GDAL vector drivers and functions, and with GEOS predicates and topological operations:


```{r, echo = TRUE} 
st_as_binary(pt3)
```

A column of simple feature geometries (`"sfc"`) is constructed as a list of `"sfg"` objects, which do not have to belong to the same Simple Features category: 

```{r, echo = TRUE} 
pt_sfc <- st_as_sfc(list(pt1, pt2, pt3))
str(pt_sfc)
```

Finally, an `"sfc"` object, a geometry column, can be added to a `data.frame` object using `st_geometry()`, which sets a number of attributes on the object and defines it as also being an `"sf"` object (the `"agg"` attribute if populated shows how observations on non-geometry columns should be understood):

```{r, echo = TRUE} 
st_geometry(DF) <- pt_sfc
(DF)
```

The **sf** package does not implement all of the Simple Features geometry categories, but geometries may be converted to the chosen subset, using for example the `gdal_utils()` function with `util="ogr2ogr", options="-nlt CONVERT_TO_LINEAR"` to convert curve geometries in an input file to linear geometries. 

Many of the functions in the **sf** package begin with `st_` as a reference to the same usage in PostGIS, where the letters were intended to symbolise space and time, but where time has not yet been implemented.

**sf** also integrates GEOS topological predicates and operations into the same framework, replacing the **rgeos** package for access to GEOS functionality. The precision and scale defaults differ between **sf** and **rgeos** slightly; both remain fragile with respect to invalid geometries, of which there are many in circulation.
\endcol

\begincol{0.48\textwidth}
```{r, echo = TRUE} 
(buf_DF <- st_buffer(DF, dist=0.3))
```
\endcol
\endcols



## Raster representations: **stars**

Like **sf**, **stars** was supported by an R Consortium grant, for scalable, spatio-temporal tidy arrays for R. 

Spatio-temporal arrays were seen as an alternative way of representing multivariate spatio-temporal data from the choices made in the **spacetime** package, where a two-dimensional data frame contained stacked positions within stacked time points or intervals. 

The proposed arrays might collapse to a raster layer if only one variable was chosen for one time point or interval. 

More important, the development of the package was extended to accommodate a backend for earth data processing in which the data are retrieved and rescaled as needed from servers, most often cloud-based servers.


This example only covers a multivariate raster taken from a Landsat 7 view of a small part of the Brazilian coast. In the first part, a GeoTIFF file is read into memory, using three array dimensions, two in planar space, the third across six bands:

```{r, echo = TRUE} 
library(stars)
fn <- system.file("tif/L7_ETMs.tif", package = "stars")
L7 <- read_stars(fn)
L7
```

```{r, echo = TRUE} 
(L7_R <- as(L7, "Raster"))
(as(L7_R, "SpatRaster"))
```

The bands can be operated on arithmetically, for example to generate a new object containing values of the normalized difference vegetation index through a function applied across the $x$ and $y$ spatial dimensions:

```{r, echo = TRUE} 
ndvi <- function(x) (x[4] - x[3])/(x[4] + x[3])
(s2.ndvi <- st_apply(L7, c("x", "y"), ndvi))
```

The same file can also be accessed using the proxy mechanism, shich creates a link to the external entity, here a file:

```{r, echo = TRUE} 
L7p <- read_stars(fn, proxy=TRUE)
L7p
```

The same function can also be applied across the same two spatial dimentions of the array, but no calculation is carried out until the data is needed and the output resolution known:

```{r, echo = TRUE} 
(L7p.ndvi = st_apply(L7p, c("x", "y"), ndvi))
```

The array object can also be split, here on the band dimension, to yield a representation as six rasters in list form:

```{r, echo = TRUE} 
(x6 <- split(L7, "band"))
```


These rasters may also be subjected to arithmetical operations, and as may be seen, explicit arithmetic on the six rasters has the same outcome as applying the same calculatiob to the three-dimensional array:

```{r, echo = TRUE} 
x6$mean <- (x6[[1]] + x6[[2]] + x6[[3]] + x6[[4]] + x6[[5]] +
              x6[[6]])/6
xm <- st_apply(L7, c("x", "y"), mean)
all.equal(xm[[1]], x6$mean)
```


### openeo

[OpenEO](http://openeo.org/about/) proposes proof-of-concept client-server API approaches. The project is under development.


### gdalcubes 

Earth Observation Data Cubes from Satellite Image Collections - extension of the **stars** proxy mechansim and the **raster** out-of-memory approach: (https://github.com/appelmar/gdalcubes_R).

Processing collections of Earth observation images as on-demand multispectral, multitemporal data cubes. Users define cubes by spatiotemporal extent, resolution, and spatial reference system and let 'gdalcubes' automatically apply cropping, reprojection, and resampling using the 'Geospatial Data Abstraction Library' ('GDAL'). 

Implemented functions on data cubes include reduction over space and time, applying arithmetic expressions on pixel band values, moving window aggregates over time, filtering by space, time, bands, and predicates on pixel values, materializing data cubes as 'netCDF' files, and plotting. User-defined 'R' functions can be applied over chunks of data cubes. The package implements lazy evaluation and multithreading. See also [this blog post](https://www.r-spatial.org//r/2019/07/18/gdalcubes1.html).


# Support

*Support* expressses the relationship between the spatial and temporal entities of observation used for capturing underlying data generation processes, and those processes themselves. 

The processes and their associated spatial and temporal scales ("footprints") and strides may not be well-understood, so the ways that we conduct observations may or may not give use good "handles" on the underlying realities. 

Since we are most often interested in drawing conclusions about the underlying realities, we should try to be aware of issues raised when we mix things up, the ecological fallacy being a typical example [@wakefield+lyons10], involving the drawing of conclusions about individuals from aggregates.

Change of support occurs when the observational entities we are using differ in their spatial and/or temporal footprint, and we need to impute or interpolate from one support to another [@gotway+young:02]. Areal interpolation is one of the pathways  [@thomas15]. 

Often we are not clear about the aggregation status of variables that we observe by entity either. In many entities, we are dealing with count aggregates. Say we have an accident count on a road segment, it is clear that if we subset the segment, we would need to impute the count to the subsegments that we have chosen. The variable is an aggregate, and subsetting should preserve the sum over all subsets rule.

```{r}
byb <- readRDS("byb.rds")
names(attributes(byb))
```

```{r}
library(sf)
st_agr(byb)
```
  
Work by [@stasch2014; @scheider2016] has shown that misunderstandings about whether variable values are constant over a segment (we really want the `gauge` to be constant), whether they are identities (`osm_id`), or whether they are measured over the whole observed time period at the point, line segment, polygon, or raster cell by counting or other aggregation, are quite prevalent. 

All `"sf"` objects have an `"agr"` attribute, set by default to unknown (`NA`) for each non-geometry column in the data frame. In this case the information is of very poor quality (many missing values, others guessed), but use can be made of the facility in other datasets.

```{r}
byb$length <- st_length(byb)
summary(byb$length)
```

```{r}
str(byb$length)
```

Unfortunately, the simple examples given in SDSR do not work. The introduction of units, shown here and in [@RJ-2016-061] also do not (yet) provide the background for issuing warnings with regard to support that were anticipated when the ideas were first considered. The idea underlying `st_agr()` has been to warn when an aggregate variable is copied across to a part of an entity as though it was a constant.


# GEOS, topology operations

(precision in **sf**, scale in **rgeos**)


### Broad Street Cholera Data

```{r echo=FALSE}
knitr::include_graphics('snowmap.png')
```

Even though we know that John Snow already had a working
hypothesis about cholera epidemics, his data remain interesting,
especially if we use a GIS to find the street distances from
mortality dwellings to the Broad Street pump in Soho in central
London. Brody et al. [-@brodyetal:00] point out that John Snow did not use
maps to *find* the Broad Street pump, the polluted water source
behind the 1854 cholera epidemic, because he associated cholera
with water contaminated with sewage, based on earlier experience.

The basic data to be used here were made available by Jim Detwiler, who had collated them for David O'Sullivan for use on the cover of O'Sullivan and Unwin [-@osullivan+unwin:03], based on earlier work by Waldo Tobler and others. The files were a shapefile of counts of deaths at front doors of houses, two shapefiles of pump locations and a georeferenced copy of the Snow map as an image; the files were registered in the British National Grid CRS. These have been converted to GPKG format. In GRASS, a suitable location was set up in this CRS and the image file was imported; the building contours were then digitised as a vector layer and cleaned.


```{r echo=FALSE}
knitr::include_graphics('brodyetal00_fig1.png')
```

We would like to find the line of equal distances shown on the extract from John Snow's map shown in Brody et al. [-@brodyetal:00] shown here, or equivalently find the distances from the pumps to the front doors of houses with mortalities following the roads, not the straight line distance. We should recall that we only have the locations of counts of mortalities, not of people at risk or of survivors.


```{r, echo=TRUE}
library(sf)
bbo <- st_read("snow/bbo.gpkg")
```

```{r, echo=TRUE, warning=FALSE}
buildings <- st_read("snow/buildings.gpkg", quiet=TRUE)
deaths <- st_read("snow/deaths.gpkg", quiet=TRUE)
sum(deaths$Num_Css)
b_pump <- st_read("snow/b_pump.gpkg", quiet=TRUE)
nb_pump <- st_read("snow/nb_pump.gpkg", quiet=TRUE)
```


As there is a small difference between the CRS values, we copy across before conducting an intersection operation to clip the buildings to the boundary, then we buffer in the buildings object (to make the roads broader).

```{r, echo=TRUE, warning=FALSE}
library(sf)
st_crs(buildings) <- st_crs(bbo)
buildings1 <- st_intersection(buildings, bbo)
buildings2 <- st_buffer(buildings1, dist=-4)
```

```{r, echo=TRUE, warning=FALSE}
plot(st_geometry(buildings2))
```

Next we create a dummy raster using **raster** with 1 meter resolution in the extent of the buildings object (note that `raster::extent()` works with **sf** objects, but the CRS must be given as a string):

```{r, echo=TRUE}
library(raster)
resolution <- 1
r <- raster(extent(buildings2), resolution=resolution, crs=st_crs(bbo)$proj4string)
r[] <- resolution
summary(r)
```

One of the `building3` component geometries was empty (permitted in **sf**, not in **sp**), so should be dropped before running `raster::cellFromPolygon()` to list raster cells in each geometry (so we need `unlist()` to assign `NA` to the in-buffered buildings):

```{r, echo=TRUE, cache=TRUE, warning=FALSE}
buildings3 <- as(buildings2[!st_is_empty(buildings2),], "Spatial")
cfp <- cellFromPolygon(r, buildings3)
is.na(r[]) <- unlist(cfp)
summary(r)
```

```{r, echo=TRUE, warning=FALSE}
plot(r)
```

Using **gdistance**, we create a symmetric transition object with an internal sparse matrix representation, from which shortest paths can be computed:
```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(gdistance)
```

```{r, echo=TRUE, cache=TRUE}
tr1 <- transition(r, transitionFunction=function(x) 1/mean(x), directions=8, symm=TRUE)
```

We need to find shortest paths from addresses with mortalities to the Broad Street pump first:

```{r, echo=TRUE, cache=TRUE, warning=FALSE}
sp_deaths <- as(deaths, "Spatial")
d_b_pump <- st_length(st_as_sfc(shortestPath(tr1, as(b_pump, "Spatial"), sp_deaths, output="SpatialLines")))
```

and then in a loop from the same addresses to each of the other pumps in turn, finally taking the minimum:

```{r, echo=TRUE, cache=TRUE, warning=FALSE}
res <- matrix(NA, ncol=nrow(nb_pump), nrow=nrow(deaths))
sp_nb_pump <- as(nb_pump, "Spatial")
for (i in 1:nrow(nb_pump)) res[,i] <- st_length(st_as_sfc(shortestPath(tr1, sp_nb_pump[i,], sp_deaths, output="SpatialLines")))
d_nb_pump <- apply(res, 1, min)
```

Because `sf::st_length()` uses **units** units, but they get lost in assigning to a matrix, we need to re-assign before testing whether the Broad Street pump is closer or not:

```{r, echo=TRUE}
library(units)
units(d_nb_pump) <- "m"
deaths$b_nearer <- d_b_pump < d_nb_pump
by(deaths$Num_Css, deaths$b_nearer, sum)
```


# Coordinate reference systems: background

The usefulness of spatial data is linked to knowing its coordinate reference system. The coordinate reference system may be geographic, usually measured in decimal degrees, or projected, layered on a known geographic CRS, usually measured in metres (planar). The underlying geographical CRS must specify an ellipsoid, with associated major and minor axis lengths:

```{r}
library(sp)
library(rgdal)
rgdal_extSoftVersion()
projInfo("ellps")
```

Other parameters should be specified, such as the prime meridian, often taken as Greenwich. Before PROJ version 6, legacy PROJ (and GDAL) used a `+datum=` tag introduced after the library migrated beyond USGS (around version 4.4). The underlying problem was not that projection and inverse projection could not be carried out between projected CRS and geograpghical CRS, but that national mapping agencies defined often many datums, keying the specification of a geographical CRS to a national or regional datum. Some of these, especially for North America, were supported, but support for others was patchy. The `+datum=` tag supported a partly informal listing of values, themselves linked to three or seven coefficient datum transformation sets, used through the `+towgs84=` tag. Coefficient lookup through the `+datum=` tag, or direct specification of coefficients through the `+towgs84=` tag became a convenient way to handle datum transformation in addition to projection and inverse projection.

The default "hub" for transformation was to go through the then newly achieved WGS84 datum. Spatial data files often encoded the geographic and projected CRS with reference to these values, in some cases using PROJ 4 strings. These used a pseudo projection `+proj=longlat` to indicate a geographical CRS, and many other possible values of `+proj=` for projected CRS.

The [Grids & Datums column](https://www.asprs.org/asprs-publications/grids-and-datums) in *Photogrammetric Engineering & Remote Sensing* gives insight into some of the peculiarities of national mapping agencies - authority is typically national but may be subnational:

```{r}
data("GridsDatums")
GridsDatums[grep("Poland", GridsDatums$country),]
```

Beyond this, the database successively developed by the European Petroleum Survey Group was copied to local CSV files for PROJ and GDAL, providing lookup by code number. From PROJ 6, GDAL no longer uses these CSV files, and PROJ makes available a SQLite database copy of the EPSG database:

```{r}
EPSG <- make_EPSG()
EPSG[grep("Poland", EPSG$note), 1:2]
EPSG[grep("GUGiK", EPSG$note), 1:2]
EPSG[grep("PL", EPSG$note), 1:2]
```

The GUGiK data downloaded online uses `"EPSG:2180"`:

```{r}
st_crs(2180)
```


Up to and including PROJ 5, downstream software, like **sf** and **rgdal**, have been able to rely on the provision of *ad-hoc* transformation capabilities, with apparently predictable consequences. Everybody knew (or should have known) that each new release of the PROJ and GDAL CSV metadata files could update transformation coefficients enough to shift outcomes a little. Everyone further chose to ignore the timestamping of coordinates, or at least of datasets; we could guess (as above) that US Census tract boundaries for 1980 must use the NAD27 datum framework - suprisingly many used NAD83 anyway (both for Boston and the North Carolina SIDS data set).

Use of KML files to provide zoom and pan for these boundaries, and now **leaflet** and **mapview** exposes approximations mercilessly. Use of coefficients of transformation of an unknown degree of approximation, and authority "googled it" was reaching its limits, or likely had exceeded them.

**sp** classes used a PROJ string to define the CRS (in an S4 `"CRS"` object):

```{r}
getClass("CRS")
```

**sf** used an S3 `"crs"` object with an integer EPSG code and a PROJ string; if instantiated from the EPSG code, both were provided. In current **sf**, the `"crs"` object has user input and wkt components, and methods to access the PROJ string and the EPSG code for backward compatibility, and from PROJ 8 and EPSG 10 we get datum ensembles:

```{r}
st_crs(4326)
```

# Modernising PROJ and issues


### PROJ

Because so much open source (and other) software uses the PROJ library and framework, many are affected when PROJ upgrades. Until very recently, PROJ has been seen as very reliable, and the changes taking place now are intended to confirm and reinforce this reliability. Before PROJ 5 (PROJ 6 was released in 2019, PROJ 7 was released in March 2020), the `+datum=` tag was used, perhaps with `+towgs84=` with three or seven coefficients, and possibly `+nadgrids=` where datum transformation grids were available. However, transformations from one projection to another first inversed to longitude-latitude in WGS84, then projected on to the target projection.


> Fast-forward 35 years and PROJ.4 is everywhere: It provides coordinate handling for almost every geospatial program, open or closed source. Today, we see a drastical  increase  in  the  need  for  high  accuracy  GNSS  coordinate  handling, especially in the agricultural and construction engineering sectors. This need for geodetic-accuracy transformations  is  not  satisfied  by "classic  PROJ.4". But with  the  ubiquity  of  PROJ.4,  we  can provide these transformations "everywhere", just by implementing them as part of PROJ.4 [@evers+knudsen17].


### Escaping the WGS84 hub/pivot: PROJ and OGC WKT2


Following the introduction of geodetic modules and pipelines in PROJ 5 [@knudsen+evers17; @evers+knudsen17], PROJ 6 moves further. Changes in the legacy PROJ representation and WGS84 transformation hub have been coordinated through the [GDAL barn raising](https://gdalbarn.com/) initiative. Crucially WGS84 often ceases to be the pivot for moving between datums. A new OGC WKT is coming, and an SQLite EPSG file database has replaced CSV files. SRS will begin to support 3D by default, adding time too as SRS change. See also [PROJ migration notes](https://proj.org/development/migration.html).

There are very useful postings on the PROJ mailing list from Martin Desruisseaux, first [proposing clarifications](https://lists.osgeo.org/pipermail/proj/2019-July/008748.html) and a [follow-up](https://lists.osgeo.org/pipermail/proj/2019-August/008750.html) including a summary:

> * "Early binding" â hub transformation technique.

> * "Late binding" â hub transformation technique NOT used, replaced by
a more complex technique consisting in searching parameters in the
EPSG database after the transformation context (source, target,
epoch, area of interest) is known.

> * The problem of hub transformation technique is independent of WGS84.
It is caused by the fact that transformations to/from the hub are
approximate. Any other hub we could invent in replacement of WGS84
will have the same problem, unless we can invent a hub for which
transformations are exact (I think that if such hub existed, we
would have already heard about it).

> The solution proposed by ISO 19111 (in my understanding) is:

> * Forget about hub (WGS84 or other), unless the simplicity of
early-binding is considered more important than accuracy.

> * Associating a CRS to a coordinate set (geometry or raster) is no
longer sufficient. A {CRS, epoch} tuple must be associated. ISO
19111 calls this tuple "Coordinate metadata". From a programmatic
API point of view, this means that getCoordinateReferenceSystem()
method in Geometry objects (for instance) needs to be replaced by a
getCoordinateMetadata() method.

This [page](http://www.sirgas.org/en/sirgas-con-network/velocity-model/) gives a picture of why the changes in PROJ matter - the arrows are in cm per year displacement:


```{r echo=FALSE}
knitr::include_graphics('VEMOS_sum.png')
```

To handle this level of detail, PROJ 7 introduces an on-demand content delivery network under user control as an alternative to dowloading many possibly unnecessary time-specific vertical and horizontal transformation grids. PROJ 8 will introduce datum ensembles, particularly for commonly used datums such as EPSG:4326, which have had (especially in Noth America) multiple, differing, instantiations as time has progressed; for now we do not know what the consequences of this will be - it is already in the EPSG database.

### Upstream software dependencies of the R-spatial ecosystem

When changes occur in upstream external software, R packages using these libraries often need to adapt, but package maintainers try very hard to shield users from any consequences, so that legacy workflows continue to provide the same or at least similar results from the same data. 

The code shown in [@asdar1; @asdar2] is almost all run nightly on a platform with updated R packages and external software. 

This does not necessarily trap all differences (figures are not compared), but is helpful in detecting impacts of changes in packages or external software. 

It is also very helpful that CRAN servers using the released and development versions of R, and with different levels of external software also run nightly checks. 

Again, sometimes changes are only noticed by users, but quite often checks run by maintainers and by CRAN alert us to impending challenges. 

Tracking the development mailing lists of the external software communities, all open source, can also show how thinking is evolving, although sometimes code tidying in external software can have unexpected consequences, breaking not **sf** or **sp** with **rgdal** or **rgeos**, but a package further downstream. 


```{r echo=FALSE}
knitr::include_graphics('sf_deps.png')
```


[@bivand14] discusses open source geospatial software stacks more generally, but here we will consider ongoing changes in PROJ, see also [@bivand:20].

[@knudsen+evers17; @evers+knudsen17] not only point out how the world has changed since a World Geodetic System of 1984 (WGS84) was adopted as a hub for coordinate transformation in PROJ, but also introduced transformation pipelines. 

In using a transformation hub, PROJ had worked adequately when the errors introduced by transforming first to WGS84 and then from WGS84 to the target coordinate reference system, but with years passing from 1984, the world has undergone sufficient tectonic shifts for errors to increase. 

In addition, the need for precision has risen in agriculture and engineering. 
So PROJ, as it was, risked ceasing to be fit for purpose as a fundamental component of the geospatial open source software stack.

Following major changes in successive iterations of the international standards for coordinate reference systems [@iso19111], PROJ is changing from preferring "late-binding" transformations, pivoting through a known transformation hub in going from input to target coordinate reference systems, to "early-binding" transformations. 

This means that the user may be offered alternative paths from input to target coordinate reference systems, some of which may go directly, and more will use higher precision transformation grids, enlarging the existing practice of using North American Datum (NAD) grids. 

In other cases, three or seven coefficient transformations may be offered, but the default fallback, where little is known about the input or target specification, may be less satisfactory than PROJ has previously offered.


### Grid CDN mechanism

The grid CDN is available at https://cdn.proj.org, and can be turned on for use in **rgdal**; I'm not aware that **sf** has made control of it available yet. Files are now stored in a single GTiff format (I think cloud-optimised).

### Transformation pipelines

In addition, the current iteration of the standard makes it more important to declare the epoch of interest of coordinates (when the position was recorded and how) and the region of interest. 

A transformation pathway may have an undefined epoch and a global span, but cannot achieve optimal precision everywhere. 

By bounding the region of interest say within a tectonic plate, and the epoch to a given five-year period, very high precision transformations may be possible. 

These choices have not so far been required explicitly, but for example matching against the `"area"` table in the database may reduce the number of transformation pathways offered dramatically.

### CRS in R before PROJ 


The **mapproj** package provided coordinate reference system and projection support for the **maps** package. From `mapproj/src/map.h`, line 20, we can see that the eccentricity of the Earth is defined as `0.08227185422`, corrresponding to the Clark 1866 ellipsoid [@iliffe]:


```{r, echo = TRUE}
ellps <- sf_proj_info("ellps")
(clrk66 <- unlist(ellps[ellps$name=="clrk66",]))
```

With a very few exceptions, projections included in `mapproj::mapproject()` use the Clarke 1866 ellipsoid, with the remainder using a sphere with the Clarke 1866 major axis radius. The function returns coordinates for visualization in an unknown metric; no inverse projections are available.

```{r, echo = TRUE}
eval(parse(text=clrk66["major"]))
eval(parse(text=clrk66["ell"]))
print(sqrt((a^2-b^2)/a^2), digits=10)
```

### Approaching and implemented changes in PROJ

- PROJ developers not only point out how the world has changed since a World Geodetic System of 1984 (WGS84) was adopted as a hub for coordinate transformation in PROJ, but also introduced transformation pipelines [@knudsen+evers17; @evers+knudsen17]. 

- In using a transformation hub, PROJ had worked adequately when the errors introduced by transforming first to WGS84 and then from WGS84 to the target coordinate reference system were acceptable, but with years passing from 1984, the world has undergone sufficient tectonic shifts for errors to increase, just as needs for accuracy sharpen. 

- In addition, the need for accuracy has risen in agriculture and engineering. 

- So PROJ, as it was, risked ceasing to be fit for purpose as a fundamental component of the geospatial open source software stack.


PROJ will also become more tightly linked to authorities responsible for the specification components. While the original well-known text (WKT1) descriptions also contained authorities, WKT2 (2019) is substantially more stringent. PROJ continues to use the European Petroleum Survey Group (EPSG) database, the local copy PROJ uses is now an SQLite database, with a large number of tables:

```{r, echo = TRUE}
sf_proj_search_paths()
```


```{r, echo = TRUE} 
library(RSQLite)
DB0 <- sf_proj_search_paths()
DB <- file.path(DB0[length(DB0)], "proj.db")
db <- dbConnect(SQLite(), dbname=DB)
cat(strwrap(paste(dbListTables(db), collapse=", ")), sep="\n")
dbDisconnect(db)
```


- The initial use of coordinate reference systems for objects defined in **sp** was based on the PROJ string representation, which built on a simplified key=value form. 

- Keys began with plus (`+`), and the value format depended on the key. 

- If essential keys were missing, some might be added by default from a file that has now been eliminated as misleading; if `+ellps=` was missing and not added internally from other keys, `+ellps=WGS84` would be added silently to refer to the World Geodetic System 1984 ellipsoid definition.

- Accurate coordinate transformation has always been needed for the integration of data from different sources, but has become much more pressing as web mapping has become available in R, through the **leaflet** package [@leaflet-package], on which **mapview** and the `"view"` mode of **tmap** build. 

- As web mapping provides zooming and panning, possible infelicities that were too small to detect as mismatches in transformation jump into prominence. 

- The web mapping workflow transforms input objects to `OGC:CRS84` (geographical CRS WGS 84, World area of relevance, WGS84 datum, visualization order) as expected by **leaflet**, then on to `EPSG:3857` (WGS 84 / Pseudo-Mercator) for display on web map backgrounds (this is carried out internally in **leaflet**). 

- Objects shown in **mapview** and **tmap** are now coerced to **sf** classes, then `st_transform()` transforms to `OGC:CRS84` if necessary (or until we are sure `EPSG:4326` never swaps axes).

### Broad Street Cholera Data



John Snow did not use maps to *find* the Broad Street pump, the polluted water source behind the 1854 cholera epidemic in Soho in central London, because he associated cholera with water contaminated with sewage, based on earlier experience [@brodyetal:00]. The basic data to be used here were made available by Jim Detwiler, who had collated them for David O'Sullivan for use on the cover of O'Sullivan and Unwin [-@osullivan+unwin:03], based on earlier work by Waldo Tobler and others.


### Where is the Broad Street pump?

We'll use the example of the location of the Broad Street pump in Soho, London, to be distributed with **sf**; the object has planar coordinates in the OSGB British National Grid projected CRS with the OSGB datum:

```{r, echo = TRUE, eval=TRUE} 
bp_file <- system.file("gpkg/b_pump.gpkg", package="sf")
b_pump_sf <- st_read(bp_file)
```

### Proj4 string degradation

Before R packages upgraded the way coordinate reference systems were represented in early 2020, our Proj4 string representation suffered degradation. Taking the Proj4 string defined in PROJ 5 for the British National Grid, there is a `+datum=OSGB36` key-value pair. But when processing this input with PROJ 6 and GDAL 3, this key is removed. Checking, we can see that reading the input string appears to work, but the output for the Proj4 string drops the `+datum=OSGB36` key-value pair, introducing instead the ellipse implied by that datum:

```{r, echo = TRUE, eval=TRUE} 
proj5 <- paste0("+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717",
 " +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs")
legacy <- st_crs(proj5)
proj6 <- legacy$proj4string
proj5_parts <- unlist(strsplit(proj5, " "))
proj6_parts <- unlist(strsplit(proj6, " "))
proj5_parts[!is.element(proj5_parts, proj6_parts)]
proj6_parts[!is.element(proj6_parts, proj5_parts)]
```

We can emulate the problem seen following the release in May 2019 of GDAL 3.0.0 using PROJ 6, by inserting the degraded Proj4 string into the Broad Street pump object. The coordinate reference system representation is now ignorant of the proper datum specification. The apparent `"proj4string"` component of the **sf** `"crs"` is used to permit packages to adapt, even though its contents are degraded.

```{r, echo = TRUE, eval=TRUE} 
b_pump_sf1 <- b_pump_sf
st_crs(b_pump_sf1) <- st_crs(st_crs(b_pump_sf1)$proj4string)
```

Why does this matter? For visualization on a web map, for example using the **mapview** package, the projected geometries are transformed to the same WGS84 ellipse and datum (`OGC:CRS84`) that were used in PROJ 4 as a transformation hub. `OGC:CRS84` is the visualization axis order equivalent of `EPSG:4326`. In **leaflet**, these are projected to Web Mercator (`EPSG:3856`). Inside `mapview()`, the `sf::st_transform()` function is used, so we will emulate this coordinate operation before handing on the geometries for display. 

However, because the one of the objects now has a degraded Proj4 string representation of its coordinate reference system, the output points, apparently transformed identically to WGS84, are now some distance apart:


```{r, echo = TRUE, eval=TRUE} 
Sys.setenv("PROJ_NETWORK"="OFF")
sf_proj_network(FALSE)
list.files(sf_proj_search_paths()[1])
b_pump_sf_ll <- st_transform(b_pump_sf, "OGC:CRS84")
list.files(sf_proj_search_paths()[1])
b_pump_sf1_ll <- st_transform(b_pump_sf1, "OGC:CRS84")
list.files(sf_proj_search_paths()[1])
sf_use_s2(FALSE)
st_distance(b_pump_sf_ll, b_pump_sf1_ll)
```


The Broad Street pump is within 1m or 2m of the green point (relative accuracy preserved) but the red point is now in Ingestre Place, because of the loss of the datum specification.

```{r, echo = TRUE, eval=TRUE} 
library(mapview)
if (sf:::CPL_gdal_version() >= "3.1.0") mapviewOptions(fgb = FALSE)
pts <- rbind(b_pump_sf_ll, b_pump_sf1_ll)
pts$CRS <- c("original", "degraded")
mapview(pts, zcol="CRS", map.type="OpenStreetMap", col.regions=c("green", "red"), cex=18)
```

### Implemented resolutions: WKT2 2019

- Once PROJ 6 and GDAL 3 had stabilized in the summer of 2019, we identified the underlying threat as lying in the advertised degradation of GDAL's `exportToProj4()` function. 

- When reading raster and vector files, the coordinate reference system representation using Proj4 strings would often be degraded, so that further transformation within R (also using GDAL/PROJ functionality) would be at risk of much greater inaccuracy than before. 

- Since then, **sf**, **sp** with **rgdal** and **raster** have adopted the 2019 version of the "Well-Known Text" coordinate reference system representation WKT2-2019 [@iso19111] instead of Proj4 strings to contain coordinate reference system definitions. 

- Accommodations have also been provided so that the S3 class `"crs"` objects used in objects defined in **sf**, and the formal S4 class `"CRS"` objects used objects defined in **sp** and **raster**, can continue to attempt to support Proj4 strings in addition, while other package maintainers and workflow users catch up.

- Following an extended campaign of repeated checking about 900 reverse dependencies (packages depending on **sp**, **rgdal** and others) and dozens of github issues, most of the consequences of the switch to WKT2 among packages have now been addressed. 

- More recently (late August 2020), 115 packages are being offered rebuilt stored objects that had included `"CRS"` objects without WKT2 definitions.

- This approach has ensured that spatial objects, whether created within R, read in from external data sources, or read as stored objects, all have WKT2 string representations of their coordinate reference systems, and for backward compatibility can represent these in addition as Proj4 strings. 

- Operations on objects should carry forward the new representations, which should also be written out to external data formats correctly. 

### Specified axis order

There is a minor divergence between **sf** and **sp** (and thus **rgdal**): in **sf**, the axis order of the CRS is preserved as instantiated, but objects do not have their axes swapped to accord with authorities unless `sf::st_axis_order()` is set `TRUEÂ´. This can appear odd, because although the representation records a northings-eastings axis order, data are treated as eastings-northings in plotting, variogram construction and so on:

```{r, echo = TRUE, eval=TRUE} 
st_crs("EPSG:4326")
```

In **sp**/**rgdal**, attempts are made to ensure that axis order is in the form termed GIS, traditional, or visualization, that is always eastings-northings. From very recent **rgdal** commits (from rev. 1060), PROJ rather than GDAL is used for instantiating CRS:

```{r, echo = TRUE, eval=TRUE} 
library(sp)
cat(wkt(CRS("EPSG:4326")), "\n")
```

The probability of confusion increases when coercing from **sf** to **sp** and vice-versa, with the representations most often remaining unchanged.


```{r, echo = TRUE, eval=TRUE} 
sf_from_sp <- st_crs(CRS("EPSG:4326"))
o <- strsplit(sf_from_sp$wkt, "\n")[[1]]
cat(paste(o[grep("CS|AXIS|ORDER", o)], collapse="\n"))
```

Both of these coercions are using the same underlying PROJ and GDAL versions, and the same PROJ metadata. Work on this question has not yet stabilized; we perhaps prefer all data to be GIS-order, but to be able to read/write from and to authority order. 

```{r, echo = TRUE, eval=TRUE} 
sp_from_sf <- as(st_crs("EPSG:4326"), "CRS")
o <- strsplit(wkt(sp_from_sf), "\n")[[1]]
cat(paste(o[grep("CS|AXIS|ORDER", o)], collapse="\n"))
```

Current thinking is to avoid the `EPSG:4326` axis order issue by recommending use of `OGC:CRS84`, which is the representation used in GeoJSON, and also known as `urn:ogc:def:crs:OGC::CRS84`. This specification is always eastings-northings:

```{r, echo = TRUE, eval=TRUE} 
cat(st_crs("OGC:CRS:84")$wkt, "\n")
```

```{r, echo = TRUE, eval=TRUE} 
cat(wkt(CRS("OGC:CRS84")), "\n")
```

### Coordinate operations

Transformation in **sf** uses code in GDAL, which in turn uses functions in PROJ; in **sp**/**rgdal**, PROJ is used directly for transformation. In order to demonstrate more of what is happening, `sf_proj_pipelines()` has been added to **sf**:


```{r, echo = TRUE, eval=TRUE}
(o <- sf_proj_pipelines(st_crs(b_pump_sf), "OGC:CRS84"))
```

```{r}
as.data.frame(o)[,c(5,8)]
```


### Areas of interest

In **sf**, areas-of-interest need to be given by the users. The provision of areas-of-interest is intended to reduce the number of candidate coordinate operations found by PROJ.

```{r, echo = TRUE, eval=TRUE} 
aoi0 <- sf_project(st_crs(b_pump_sf), "OGC:CRS84", matrix(unclass(st_bbox(b_pump_sf)), 2, 2, byrow=TRUE))
aoi <- c(t(aoi0 + c(-0.1, +0.1)))
(o_aoi <- sf_proj_pipelines(st_crs(b_pump_sf), "OGC:CRS84", AOI=aoi))
```


```{r}
as.data.frame(o_aoi)[,c(5,8)]
```


`sf::sf_proj_pipelines()` accesses the PROJ metadata database to search through candidate coordinate operations, ranking them by accuracy, returning a data frame of operations. When an area-of-interest is provided, candidates not intersecting it are dropped. Coordinate operations that cannot be instantiated because of missing grids are also listed. Here without an area-of-interest: `r formatC(nrow(o), format="d")` candidate operations are found when the WKT string contains datum information. Of these, `r formatC(sum(o[["instantiable"]]), format="d")` may be instantiated, with `r formatC(sum(o[["grid_count"]]), format="d")` needing a grid. `r formatC((nrow(o) - nrow(o_aoi)), format="d")` operations cease to be candidates if we use an area-of-interest.

### Coordinate operations

In **sp**/**rgdal**, the coordinate operation last used is returned, and can be retrieved using `rgdal::get_last_coordOp()`; coordinate operations are represented as pipelines [@knudsen+evers17; @evers+knudsen17], introduced in PROJ 5 and using the PROJ key-value pair notation:

```{r, echo = TRUE, eval=TRUE} 
(helm <- as.data.frame(o_aoi)[o_aoi$accuracy==2, "definition"])
b_pump_sf_ll <- st_transform(b_pump_sf, "OGC:CRS84", pipe=helm, aoi=aoi)
```

Here we can see that an inverse projection from the specified Transverse Mercator projection is made to geographical coordinates, followed by a seven-parameter Helmert transformation to WGS84 ellipsoid and datum. The parameters are contained in the best instantiable coordinate operation retrieved from the PROJ database. The `+push +v_3` and `+pop +v_3` operations are used when only horizontal components are needed in the Helmert transformation.

```{r}
(o_aoi1 <- sf_proj_pipelines(st_crs(b_pump_sf1), "OGC:CRS84", AOI=aoi))
```

```{r, echo = TRUE, eval=TRUE} 
b_pump_sf1_ll <- st_transform(b_pump_sf1, "OGC:CRS84")
st_distance(b_pump_sf_ll, b_pump_sf1_ll)
```

Going on to the case of the degraded representation, only `r formatC(nrow(o_aoi1), format="d")` operation is found, with only ballpark accuracy. With our emulation of the dropping of `+datum=` support in GDAL's `exportToProj4()`, we see that the coordinate operation pipeline only contains the inverse projection step, accounting for the observed shift of the Broad Street pump to Ingestre Place.

### Using the content download network to access grids

Finally, **sp**/**rgdal** may use the provision of on-demand downloading of transformation grids to provide more accuracy (CDN, from PROJ 7, https://cdn.proj.org ). Before finding and choosing to use a coordinate operation using an on-demand downloaded grid, the designated directory is empty:


```{r, echo = TRUE, eval=TRUE} 
sf_proj_network()
```

```{r, echo = TRUE, eval=TRUE} 
Sys.setenv("PROJ_NETWORK"="ON")
sf_proj_network(TRUE)
```

Using the CDN, all the candidate operations are instantiable, and the pipeline now shows a horizontal grid transformation rather than a Helmert transformation.




```{r, echo = TRUE, eval=TRUE} 
(og <- sf_proj_pipelines(st_crs(b_pump_sf), "OGC:CRS84", AOI=aoi))
```

```{r, echo = TRUE, eval=TRUE} 
list.files(sf_proj_search_paths()[1])
b_pump_sf_llgd <- st_transform(b_pump_sf, "OGC:CRS84")
list.files(sf_proj_search_paths()[1])
```

```{r}
library(rgdal)
is_proj_CDN_enabled()
b_pump_sp_llgd <- spTransform(as(b_pump_sf, "Spatial"), "OGC:CRS84")
list.files(sf_proj_search_paths()[1])
DB <- file.path(DB0[1], "cache.db")
db <- dbConnect(SQLite(), dbname=DB)
cat(strwrap(paste(dbListTables(db), collapse=", ")), sep="\n")
dbReadTable(db, "chunks")
dbDisconnect(db)
b_pump_sf_llgdsp <- st_as_sf(b_pump_sp_llgd)
```



Now the downloaded grid is cached in the database in the designated CDN directory, and may be used for other transformations using the same operation.


```{r, echo = TRUE, eval=TRUE} 
Sys.setenv("PROJ_NETWORK"="OFF")
sf_proj_network(FALSE)
c(st_distance(b_pump_sf_ll, b_pump_sf1_ll), st_distance(b_pump_sf_ll, b_pump_sf_llgd), st_distance(b_pump_sf_ll, b_pump_sf_llgdsp))
```

Once again, the distance between the point transformed from the **sf** object as read from file and the point with the degraded coordinate reference system emulating the effect of the change in behaviour of GDAL's `exportToProj4()` in GDAL 6 and later is about 125m. Using the CDN shifts the output point by 1.7m, here using both the new **s2** and legacy interfaces for measurements in **sf** using geographical coordinates [@s2-package]:

```{r, echo = TRUE, eval=TRUE} 
sf_use_s2(TRUE)
c(st_distance(b_pump_sf_ll, b_pump_sf1_ll), st_distance(b_pump_sf_ll, b_pump_sf_llgd), st_distance(b_pump_sf_ll, b_pump_sf_llgdsp))
sf_use_s2(FALSE)
```

To be sure that **sf** uses the CDN, it appears that you need to set `Sys.setenv("PROJ_NETWORK"="ON")` before **sf** is loaded and attached. This does not apply to **rgdal**, but possibly reflects the fact that **rgdal** uses PROJ directly, while `sf::st_transform()` uses PROJ through GDAL (and `sf::sf_proj_pipelines()` uses PROJ directly).

### CRS representation: status

- Although it appears that most of the consequences of the change in representation of coordinate reference systems from Proj4 to WKT2 strings have now been addressed, we still see signs on the mailing list and on Twitter that users, naturally directing their attention to their analytical or visualization work, may still be confused. 

- The extent of the spatial cluster of R packages is so great that it will undoubtedly take time before the dust settles. 

- However, we trust that the operation of upgrading representations is now largely complete. 

- Multiple warnings issued in **sp** workflows, now noisily drawing attention to possible degradations in workflows, will by default be muted when **sp** 1.5 and **rgdal** 1.6 are released.



### R's `sessionInfo()`

```{r sI, echo = TRUE}
sessionInfo()
```
